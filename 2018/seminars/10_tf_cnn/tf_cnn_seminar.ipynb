{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks and images in Tensorflow\n",
    "** How too install tensorflow: https://www.tensorflow.org/install/ **\n",
    "\n",
    "* `pip install tensorflow` -- **cpu-only** version for Linux & Mac OSX\n",
    "* if you want GPU support try -- `pip install tensorflow-gpu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this notebook:\n",
    "```\n",
    "conda create -n py36_tensorflow python=3.6 anaconda\n",
    "source activate py36_tensorflow\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab link: https://colab.research.google.com/drive/18xjvLspViCwTUXTBNiz_xKxlUblQuGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About CNNs\n",
    "Convolutional layers extract features - quantitative representations of some attributes. \n",
    "\n",
    "After the extraction you can use these features for classification, for example.\n",
    "\n",
    "<img src=\"img/act.png\" width=\"800\">\n",
    "\n",
    "## Let's look at popular architectures.\n",
    "\n",
    "### VGG\n",
    "\n",
    "<img src=\"img/vgg.png\" width=\"600\">\n",
    "\n",
    "### ResNet (Shortcut + Batch Normalization)\n",
    " \n",
    "<img src=\"img/resnet.png\" width=\"800\">\n",
    " \n",
    "### GoogleNet (Predict classes for many times)\n",
    " \n",
    "<img src=\"img/gln.png\" width=\"800\">\n",
    "\n",
    "\n",
    "## Deeper layer $\\to$ more complex features.\n",
    "\n",
    "<img src=\"img/feat.png\" width=\"800\">\n",
    "\n",
    "## In practice it is easier to learn pre-trained NN (Fine-Tuning)\n",
    "\n",
    "<img src=\"img/ft.jpg\" width=\"600\">\n",
    "\n",
    "\n",
    "## Dark Magic \n",
    "\n",
    "<img src=\"img/dm.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's play with tensorflow. ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work?\n",
    "1. define placeholders where you'll send inputs;\n",
    "2. make symbolic graph: a recipe for mathematical transformation of those placeholders;\n",
    "3. compute outputs of your graph with particular values for each placeholder\n",
    "  * output.eval({placeholder:value}) \n",
    "  * s.run(output, {placeholder:value})\n",
    "\n",
    "* So far there are two main entities: \"placeholder\" and \"transformation\"\n",
    "* Both can be numbers, vectors, matrices, tensors, etc.\n",
    "* Both can be int32/64, floats of booleans (uint8) of various size.\n",
    "\n",
    "* You can define new transformations as an arbitrary operation on placeholders and other transformations\n",
    " * tf.reduce_sum(tf.arange(N)\\**2) are 3 sequential transformations of placeholder N\n",
    " * There's a tensorflow symbolic version for every numpy function\n",
    "   * `a+b, a/b, a**b, ...` behave just like in numpy\n",
    "   * np.mean -> tf.reduce_mean\n",
    "   * np.arange -> tf.range\n",
    "   * np.cumsum -> tf.cumsum\n",
    "   * If if you can't find the op you need, see the [docs](https://www.tensorflow.org/api_docs/python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note\n",
    "Functions are similar to numpy. But we should remember that we declarate computational graph, and these functions will compute only while session is on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - placeholders\n",
    "`Placeholder is a tensor of fix size.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = tf.placeholder('int64', name=\"input_to_your_name\")\n",
    "# N square.\n",
    "result_0 = tf.pow(N,2)\n",
    "# Sum of squares of numbers up to N.\n",
    "result_1 = tf.reduce_sum((tf.range(N)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All initial parameters are submited to session as dict ({Placeholder_name: value})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(result_0, {N:100}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(result_1, {N:100}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 - placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype='float32', shape=(10,))\n",
    "y = tf.placeholder(dtype='float32', shape=(10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "func = 100 * tf.multiply(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = np.arange(10).astype('float32')\n",
    "print(func.eval({x:dummy, y:dummy}, session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 - gradients\n",
    "It is really easy to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scalar = tf.placeholder('float32')\n",
    "scalar_squared = my_scalar**2\n",
    "\n",
    "derivative = tf.gradients(scalar_squared, my_scalar)[0]\n",
    "\n",
    "x = np.linspace(-3,3)\n",
    "x_squared, x_squared_der = sess.run(\n",
    "    [scalar_squared, derivative],\n",
    "    {my_scalar:x})\n",
    "\n",
    "plt.plot(x, x_squared, label=\"x^2\")\n",
    "plt.plot(x, x_squared_der, label=\"derivative\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note\n",
    "This way we can automatically compute gradients even for nasty functions. It is important, that this is **not a numerical differentiation**, there are formulas for derivatives within tensorflow, and it goes through graph vertexes and calculate the derivative of composite function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4 - variables\n",
    "\n",
    "The inputs and transformations have no value outside function call. This isn't too comfortable if you want your model to have parameters (e.g. network weights) that are always present, but can change their value over time.\n",
    "\n",
    "Tensorflow solves this with `tf.Variable` objects.\n",
    "* You can assign variable a value at any time in your graph\n",
    "* Unlike placeholders, there's no need to explicitly pass values to variables when `s.run(...)`-ing\n",
    "* You can use variables the same way you use transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_1 = tf.Variable(initial_value=np.ones(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT one:\n",
    "# initialize variable(s) with initial values\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#evaluating shared variable (outside symbolicd graph)\n",
    "print(\"initial value\", sess.run(v_1))\n",
    "\n",
    "# within symbolic graph you use them just as any other inout or transformation, not \"get value\" needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5 - logistic regression with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits(2)\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "print(\"y [shape - %s]:\" % (str(y.shape)), y[:10])\n",
    "print(\"X [shape - %s]:\" % (str(X.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0].reshape([8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(np.zeros((64, 1)), dtype='float32')\n",
    "input_X = tf.placeholder('float32', shape=(None, 64))\n",
    "input_y = tf.placeholder('float32', shape=(None, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.matmul(input_X, weights)\n",
    "# hand-made sigmoid\n",
    "predicted_y = 1. / (1. + tf.exp(-1. * logits))\n",
    "epsilon = 0.005\n",
    "# hand-made logloss\n",
    "loss = -1. * tf.reduce_sum( \n",
    "    tf.multiply(input_y, tf.log( epsilon + predicted_y)) \n",
    "    + tf.multiply((1 - input_y), tf.log(1 + epsilon - predicted_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: Why do we need this epsilon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use optimizer instead of hand-made gradient descent.\n",
    "optimizer_step = (\n",
    "    tf.train.GradientDescentOptimizer(0.001, use_locking=True)\n",
    "    .minimize(loss, var_list=weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a bit of theano-like logic :)\n",
    "# <compile function that takes X and y, returns log loss and updates weights>\n",
    "def train_function(X, y, s=sess):    \n",
    "    X = X.reshape((batch_size, 64))\n",
    "    y = y.reshape((batch_size, 1))\n",
    "    s.run(optimizer_step, {input_X: X, input_y: y})\n",
    "    log_loss = s.run(loss, {input_X: X, input_y: y})\n",
    "    return log_loss\n",
    "    \n",
    "predict_function = lambda X: sess.run(tf.round(predicted_y), {input_X: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    batch_indixes = random.sample(\n",
    "        range(0, X_train.shape[0]), batch_size)\n",
    "    X_sample = X_train[batch_indixes]\n",
    "    y_sample = y_train[batch_indixes]\n",
    "    loss_i = train_function(X_sample, y_sample)\n",
    "    print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "    print(\"train auc:\", roc_auc_score(y_train, predict_function(X_train)))\n",
    "    print(\"test auc:\", roc_auc_score(y_test, predict_function(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our model weights\n",
    "print (\"resulting weights:\")\n",
    "ax = sns.heatmap(weights.eval(session=sess).reshape([8, 8]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: why such weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6 - real-but-toy CNN in TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define some standart like arhitecture, like \n",
    "`conv-pool-conv-pool-dense-dense-everybody`\n",
    "\n",
    "1) Convolutional Layer #1: Applies 32 5x5 filters (extracting 5x5-pixel subregions), with ReLU activation function\n",
    "\n",
    "2) Pooling Layer #1: Performs max pooling with a 2x2 filter and stride of 2 (which specifies that pooled regions do not overlap)\n",
    "\n",
    "3) Convolutional Layer #2: Applies 64 5x5 filters, with ReLU activation function\n",
    "\n",
    "4) Pooling Layer #2: Again, performs max pooling with a 2x2 filter and stride of 2\n",
    "\n",
    "5) Dense Layer #1: 1,024 neurons, with dropout regularization rate of 0.4 (probability of 0.4 that any given element will be dropped during training)\n",
    "\n",
    "6) Dense Layer #2 (Logits Layer): 10 neurons, one for each digit target class (0–9). \n",
    "\n",
    "As we use TF for NN, let's use some of predefined layers.\n",
    "\n",
    "** More details about standard layres here: https://www.tensorflow.org/tutorials/layers **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training and eval data\n",
    "train_data = mnist.train.images # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train shape', train_data.shape, ', target shape', train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = train_data.reshape((train_data.shape[0], 28, 28))\n",
    "fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(20, 20))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(visible[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our model defenition (TF best practices)\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        inputs=conv1, \n",
    "        pool_size=[2, 2], \n",
    "        strides=2)\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        inputs=conv2, \n",
    "        pool_size=[2, 2], \n",
    "        strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    dense = tf.layers.dense(\n",
    "        inputs=pool2_flat, \n",
    "        units=1024, \n",
    "        activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=dense, \n",
    "        rate=0.4, \n",
    "        training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about Estimators: \n",
    "- https://www.tensorflow.org/programmers_guide/estimators\n",
    "- https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hooks = None\n",
    "if False:  # for console output\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "        tensors=tensors_to_log, every_n_iter=1000)\n",
    "    hooks = [logging_hook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": train_data},  # data source\n",
    "    y=train_labels,       # labels source\n",
    "    batch_size=100,       # batch size\n",
    "    num_epochs=None,      # data limit (if u want to train prespecified number of epochs)\n",
    "    shuffle=True)         # shuffle flag\n",
    "mnist_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=2000,  # change here for better results/time, (~10min for 2k steps)\n",
    "    hooks=hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Example 7 - tensorboard\n",
    "\n",
    "TensorBoard is a suite of web applications for inspecting and understanding your TensorFlow runs and graphs.\n",
    "\n",
    "Go to console and activate your python environment with tensorflow installed, then execute command:\n",
    "\n",
    "`tensorboard -logdir /tmp/mnist_convnet_model`.\n",
    "\n",
    "Now, go to `127.0.0.1:6006` to see the TensorBoard\n",
    "\n",
    "More info: https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Example 8 - practise notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "for i in range(0, NUM_EPOCHS):\n",
    "    (mnist_classifier\n",
    "     .train(input_fn=train_input_fn, steps=2000, hooks=hooks)\n",
    "     .evaluate(input_fn=eval_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If u want even more TF - https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Обратная связь ** \n",
    "  * оцените <a href=\"https://goo.gl/forms/kYZuyAQLuwo8szce2\"> семинар </a>\n",
    "  * оставьте <a href=\"https://goo.gl/forms/zeZiu1fSgrpPGp6T2\"> отзыв </a> о лекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
