{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural networks and Keras in Tensorflow\n",
    "** How too install tensorflow: https://www.tensorflow.org/install/ **\n",
    "\n",
    "* `pip install tensorflow` -- **cpu-only** version for Linux & Mac OSX\n",
    "* if you want GPU support try -- `pip install tensorflow-gpu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook:\n",
    "```\n",
    "conda create -n py36_tensorflow python=3.6 anaconda\n",
    "source activate py36_tensorflow\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About RNNs\n",
    "\n",
    "Recurrent layers extract features and hidden connections along a sequence.\n",
    "\n",
    "## Let's look at popular varieties.\n",
    "\n",
    "#### PureRNN\n",
    "<img src=\"img/pure_rnn.png\" width=\"600\">\n",
    "\n",
    "\n",
    "#### LSTM\n",
    "article - http://www.bioinf.jku.at/publications/older/2604.pdf\n",
    "<img src=\"img/lstm.png\" width=\"600\">\n",
    "\n",
    "#### GRU\n",
    "article - https://arxiv.org/abs/1406.1078\n",
    "<img src=\"img/gru.png\" width=\"600\">\n",
    "\n",
    "## Try power of the RNNs\n",
    "\n",
    "<img src=\"img/rnns.jpeg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Karpathy blog:  http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "#### Denny Britz blog: http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "#### Seq2seq example: https://github.com/Scitator/YATS2S/blob/versions/tf_1.2/seq2seq_example_tatoeba_inference.ipynb\n",
    "#### About Keras: https://habrahabr.ru/company/ods/blog/325432/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal for today:\n",
    "1. Create RNN-model (many2one), LM like\n",
    "2. Use it to generate text (logic: many many2one = many2many)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab link: https://colab.research.google.com/drive/1yhe2ZbqMC70rHa8GhaOdopc1wwcgGbtS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# when keras comes to tensorflow :slava_2:\n",
    "layers = tf.keras.layers\n",
    "models = tf.keras.models\n",
    "optimizers = tf.keras.optimizers\n",
    "callbacks = tf.keras.callbacks\n",
    "get_file = tf.keras.utils.get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. We need some corpora..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EN\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt') # amazon!\n",
    "text = io.open(path, encoding='utf-8').read().lower().replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('corpus length:', len(text))\n",
    "print('text sample: ', text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing - encode all your data to numbers\n",
    "3 ways:\n",
    "1. word-level: stemming, lemmatization -> tokenization\n",
    "    + (+) w2v embeddings can be used (GoogleVec, Glove, etc)\n",
    "    - (-/+) a bit complex preprocessing\n",
    "    - (-) OOV words\n",
    "2. char-level: tokenize them all!\n",
    "    - (+) quite simple\n",
    "    - (+) no OOV\n",
    "    - (-/+) complex model\n",
    "3. subword-level: ngrams or BPE\n",
    "    - (+) no OOV (only sometimes)\n",
    "    - (-/+) complex preprocessing\n",
    "    - (-/+) complex model\n",
    "    - pretrained w2v - https://github.com/bheinzerling/bpemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vocabulary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are lazy enoght, let's use char level\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model input creation - sequences of same len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Finally, vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Now, let's create simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars)))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "optimizer = optimizers.RMSprop(lr=0.01)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you have graphviz and pydot installed correctly.\n",
    "# Beautifull Keras-model visualization.\n",
    "# tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Our sample method\n",
    "Takes logits and outputs 'most probable' variant\n",
    "\n",
    "About temperature: https://cs.stackexchange.com/questions/79241/what-is-temperature-in-lstm-and-neural-networks-generally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training callback\n",
    "\n",
    "Takes current model and generates different texts with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_callback = callbacks.LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train them all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x, y,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt  # noqa: E402\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "import seaborn as sns  # noqa: E402\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "\n",
    "def create_if_need(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def plot_rutine(\n",
    "        history, metric,\n",
    "        legend=None, additional_info=None,\n",
    "        show=False, save_dir=None,\n",
    "        last_steps=None, last_steps_reference=None):\n",
    "    title = 'model {}'.format(metric)\n",
    "    if additional_info is not None:\n",
    "        title += \" {}\".format(additional_info)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "    if save_dir is not None:\n",
    "        filename = \"{}/{}\".format(save_dir, metric)\n",
    "        if additional_info is not None:\n",
    "            filename += \"_{}\".format(str(additional_info))\n",
    "        filename += \".png\"\n",
    "        if legend is not None:\n",
    "            plt.savefig(filename, format='png', dpi=300,\n",
    "                        bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        else:\n",
    "            plt.savefig(filename, format='png', dpi=300)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    if last_steps is not None and isinstance(last_steps, float):\n",
    "        last_steps = int(last_steps * len(history[metric]))\n",
    "\n",
    "    if last_steps is not None and isinstance(last_steps, int) and last_steps_reference is not None:\n",
    "        last_steps_history = {metric: history[metric][-last_steps:]}\n",
    "        val_metric = \"val_{}\".format(metric)\n",
    "        if val_metric in history.keys():\n",
    "            last_steps_history[val_metric] = history[val_metric][-last_steps:]\n",
    "        last_steps_reference(\n",
    "            history=last_steps_history,\n",
    "            metric=metric,\n",
    "            additional_info=\"last_steps\",\n",
    "            show=show,\n",
    "            save_dir=save_dir)\n",
    "\n",
    "\n",
    "def plot_bimetric(\n",
    "        history, metric,\n",
    "        additional_info=None,\n",
    "        show=False, save_dir=None, last_steps=None):\n",
    "    plt.figure()\n",
    "\n",
    "    if \"log_epochs\" in history:\n",
    "        plt.plot(history[\"log_epochs\"], history[metric])\n",
    "        plt.plot(history[\"log_epochs\"], history['val_{}'.format(metric)])\n",
    "    else:\n",
    "        plt.plot(history[metric])\n",
    "        plt.plot(history['val_{}'.format(metric)])\n",
    "\n",
    "    lgn = plt.legend(['train', 'val'], loc='center left',\n",
    "                     bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    plot_rutine(\n",
    "        history, metric,\n",
    "        legend=lgn, additional_info=additional_info,\n",
    "        show=show, save_dir=save_dir,\n",
    "        last_steps=last_steps, last_steps_reference=plot_bimetric)\n",
    "\n",
    "\n",
    "def plot_unimetric(\n",
    "        history, metric,\n",
    "        additional_info=None,\n",
    "        show=False, save_dir=None, last_steps=None):\n",
    "    plt.figure()\n",
    "\n",
    "    if \"log_epochs\" in history:\n",
    "        plt.plot(history[\"log_epochs\"], history[metric])\n",
    "    else:\n",
    "        plt.plot(history[metric])\n",
    "\n",
    "    plot_rutine(\n",
    "        history, metric,\n",
    "        legend=None, additional_info=additional_info,\n",
    "        show=show, save_dir=save_dir,\n",
    "        last_steps=last_steps, last_steps_reference=plot_unimetric)\n",
    "\n",
    "\n",
    "def plot_all_metrics(\n",
    "        history,\n",
    "        show=False, save_dir=None, last_steps=None):\n",
    "    if save_dir is not None:\n",
    "        create_if_need(save_dir)\n",
    "\n",
    "    bimetrics = []\n",
    "    for metric in history.keys():\n",
    "        if metric.startswith(\"val_\"):\n",
    "            bimetrics.append(metric[4:])\n",
    "\n",
    "    for metric in history.keys():\n",
    "        if not metric.startswith(\"val_\"):\n",
    "            if metric in bimetrics:\n",
    "                plot_bimetric(\n",
    "                    history, metric,\n",
    "                    additional_info=None, show=show,\n",
    "                    save_dir=save_dir, last_steps=last_steps)\n",
    "            else:\n",
    "                plot_unimetric(\n",
    "                    history, metric,\n",
    "                    additional_info=None, show=show,\n",
    "                    save_dir=save_dir, last_steps=last_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_all_metrics(history.history, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
